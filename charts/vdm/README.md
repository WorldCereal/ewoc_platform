# Deploy Visualization Dissemination Module  Application (VDM)

## Requirements
- Access to kubectl
- Client vdm is created in keycloak and well configured
- Get the Client secret generated by keycloak
- Kong is up and running
- export-env.sh is sourced

## Steps

1. Copy the VDM repository on github https://github.com/WorldCereal/gisat-env-world-cereal

2. Create the vdm namespace that is going to host all the things related to this application.
```kubectl create ns vdm```

3. Create the secret ```harborcs`Ì€`` that allow to pull the image from private registry.
```kubectl create secret -n vdm docker-registry harborcs --docker-server=YOUR_REGISTRY --docker-username=REGISTRY_USERNAME --docker-password="REGISTRY_PASSWORD"```

4. Create the PVC for database persistance. 
For this step use the file in ```pvc/pg-data-pvc.yaml``` and apply it with the following command:
```kubectl apply -f pg-data-pvc.yaml -n vdm```
You can update the size of the PVC according your needs by changing the storage value in this file. 

5. Change parameters in the helm chart according your needs.
You will have to change some configuration elements in configmaps, (domain name, password..). 
**Don't forget to add the nodeSelector to restrict the domain of execution of the deployed pods**

6. Once all the parameters match with your environement, play the following command that is going to install the chart onto the vdm namespace.
```cd helm-chart``` then ```helm install vdm -n vdm .``` after execution helm should display a validation message. 
Wait two minutes then check that all the pods are running. If a pod is in error state, that means that something is missing or wrong in your configuration. (Step 5) So you have to delete the deployment and check your configuration. To delete the deployment use ```helm delete -n vdm vdm```

7. Now that the vdm application is up and running, you have to setup the ingress to expose it.
To do so, use the ingress provided here ```ingress.yaml```. Update values according your domain name and ssl needs then 
use the following command : ```kubectl apply -n vdm -f ingress.yaml```.
This ingress expose two kind of route. One on **/** that allow to load the page from the webserver app-wcpdr, 
and /backend that expose on **/backend** an api pointing to the backend (be).

8. Now that the ingress is deployed, it is required to protect the access to this application by using kong OIDC authentification and keycloak authentification.
Check that you have set the hostname and the client secret of VDM on ```export-env.sh``` and ```source export-env.sh```
Then: 
```make kong```
**NB: if you change the ingress name, change the kong endpoint because the route name is build from the ingress name**

9. Check that the application is protected by the OIDC authentification by trying to access it from you browser.

## Information
VDM is composed of 4 deployments.
- redis used for caching
- postgresql
- app-wcpdr which is the front end
- be or backend which is composed of 2 containers mapproxy and be. The containers shares 4 PV mounted in each one.

be and postgres uses PV. 
postgres uses pg-data PV. 
be uses mapproxy-cache mapproxy-conf mapproxy-datasource mapproxy-seed PV's.

## Troubleshooting

- When deploying postgresql for the fist time, it's possible to have issues with pods connection to the db.
If it's the case, connect to the postgresql pods et delete the `lost+found` folder in the mounted PVC directory.

- If the front-end do not provides tiles, it's possible that the shared PVC in the `be` pods has some rights issues. Mapproxy is running as uid 1000 whereas the be container is root.
To solve this, connect in be pod and select be container then exec a recursive chown to give rights to uid 1000.

## Specificity
On the VDM Api ingress route, we had to add a specific plugin `request-transformer`.
This plugin is used to delete the Authorization header because OIDC and the API use this very same header which create incompatibilty which leaded to failed authentication by Keycloak/Kong.